{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0932463",
   "metadata": {},
   "source": [
    "## üìò Documentation for Google Gemini API with LangChain\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560311e6",
   "metadata": {},
   "source": [
    "This guide explains how to use **Google‚Äôs Gemini models** through **LangChain**.\n",
    "We‚Äôll cover setup, code explanation, and why the environment variable name must be exactly `GOOGLE_API_KEY`.\n",
    "\n",
    "---\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "1. **Python installed** (>=3.8 recommended).\n",
    "\n",
    "2. Install required libraries:\n",
    "\n",
    "   ```bash\n",
    "   pip install langchain-google-genai python-dotenv\n",
    "   ```\n",
    "\n",
    "3. Get a **Google API Key** from [Google AI Studio](https://aistudio.google.com/app/apikey).\n",
    "\n",
    "---\n",
    "\n",
    "### Setting up the `.env` File\n",
    "\n",
    "To keep your API key secure, we‚Äôll store it in a `.env` file.\n",
    "\n",
    "1. Create a file named `.env` in your project folder.\n",
    "2. Add your key like this:\n",
    "\n",
    "   ```env\n",
    "   GOOGLE_API_KEY=\"your_api_key_here\"\n",
    "   ```\n",
    "\n",
    "#### Important\n",
    "\n",
    "* The variable **must** be named exactly `GOOGLE_API_KEY`.\n",
    "* If you use any other name (e.g., `MY_KEY`), the code will fail.\n",
    "\n",
    "---\n",
    "\n",
    "### Why must the name be `GOOGLE_API_KEY`?\n",
    "\n",
    "LangChain integrations follow a **convention-over-configuration rule**:\n",
    "Each provider (OpenAI, Google, etc.) has a **predefined environment variable name** that LangChain automatically looks for.\n",
    "\n",
    "For Google Gemini, LangChain‚Äôs internal code does:\n",
    "\n",
    "```python\n",
    "os.getenv(\"GOOGLE_API_KEY\")\n",
    "```\n",
    "\n",
    "* If it finds a value ‚Üí it uses that key for authentication.\n",
    "* If it doesn‚Äôt ‚Üí you‚Äôll get an authentication error.\n",
    "\n",
    "So unless you **manually pass a key** in the code, the variable name must be exactly:\n",
    "\n",
    "```\n",
    "GOOGLE_API_KEY\n",
    "```\n",
    "\n",
    "That‚Äôs why renaming it (e.g., `MY_KEY`) won‚Äôt work.\n",
    "\n",
    "---\n",
    "\n",
    "### Code Explanation\n",
    "\n",
    "```python\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "```\n",
    "\n",
    "* `langchain_google_genai`: Gives access to Google Gemini models.\n",
    "* `dotenv`: Loads values from `.env`.\n",
    "* `os`: Lets Python read environment variables.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "load_dotenv()\n",
    "```\n",
    "\n",
    "* Loads `.env` and makes `GOOGLE_API_KEY` available to the program.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "model = ChatGoogleGenerativeAI(model='gemini-2.5-flash')\n",
    "```\n",
    "\n",
    "* Creates a Gemini model instance (`gemini-2.5-flash` = fast variant).\n",
    "* LangChain automatically uses `GOOGLE_API_KEY` from your environment.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "result = model.invoke(\"current finance minister of india\")\n",
    "print(result.content)\n",
    "```\n",
    "\n",
    "* Sends the query to Gemini.\n",
    "* `result.content` prints the AI‚Äôs response.\n",
    "\n",
    "\n",
    "### Below is the complete code for the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f4dbf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current Finance Minister of India is **Nirmala Sitharaman**.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model='gemini-2.5-flash')\n",
    "\n",
    "result = model.invoke(\"current finance minister of india\")\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29282e41",
   "metadata": {},
   "source": [
    "## üìò Documentation for Hugging Face API  with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8055c2",
   "metadata": {},
   "source": [
    "This guide explains how to use **Hugging Face models** with **LangChain** through Hugging Face Inference Endpoints.\n",
    "We‚Äôll break down the setup, explain the libraries used, what an **endpoint** really means, why `ChatHuggingFace` is needed, and why the `.env` variable name must be exact.\n",
    "\n",
    "---\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "1. **Python installed** (>=3.8 recommended).\n",
    "\n",
    "2. Install required libraries:\n",
    "\n",
    "   ```bash\n",
    "   pip install langchain-huggingface python-dotenv\n",
    "   ```\n",
    "\n",
    "3. A **Hugging Face Access Token**:\n",
    "\n",
    "   * Sign in to [Hugging Face](https://huggingface.co/).\n",
    "   * Go to **Settings ‚Üí Access Tokens**.\n",
    "   * Create a token with **read access**.\n",
    "\n",
    "---\n",
    "\n",
    "### Setting up the `.env` File\n",
    "\n",
    "To keep your token secure, store it in a `.env` file.\n",
    "\n",
    "1. Create a `.env` file in your project folder.\n",
    "2. Add your token like this:\n",
    "\n",
    "   ```env\n",
    "   HUGGINGFACEHUB_API_TOKEN=\"your_hf_token_here\"\n",
    "   ```\n",
    "\n",
    "#### Important\n",
    "\n",
    "* The variable **must** be named `HUGGINGFACEHUB_API_TOKEN`.\n",
    "* If you use another name (like `HF_TOKEN`), LangChain will not detect it automatically.\n",
    "\n",
    "üëâ **Historical Note**:\n",
    "\n",
    "* A few months ago, LangChain also supported the variable name `HUGGINGFACEHUB_ACCESS_TOKEN`.\n",
    "* Now it has been standardized to `HUGGINGFACEHUB_API_TOKEN`.\n",
    "* At the time you follow this guide, **always check the latest LangChain docs**, since conventions may change again.\n",
    "\n",
    "---\n",
    "\n",
    "### Libraries & Classes Explained\n",
    "\n",
    "```python\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "```\n",
    "\n",
    "#### 1. `langchain_huggingface`\n",
    "\n",
    "LangChain‚Äôs Hugging Face integration library.\n",
    "\n",
    "* **`HuggingFaceEndpoint`**\n",
    "\n",
    "  * Connects your code to a Hugging Face **inference endpoint** (the online API to use a model).\n",
    "  * You give it the `repo_id` (model name) and your API token.\n",
    "  * It sends your request to Hugging Face‚Äôs servers and brings back the response.\n",
    "\n",
    "* **`ChatHuggingFace`**\n",
    "\n",
    "  * A wrapper that makes the raw model behave like a **chatbot**.\n",
    "  * Instead of handling raw API formats, you can simply call `.invoke(\"your message\")`.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. `dotenv` (from `python-dotenv`)\n",
    "\n",
    "* Loads variables from your `.env` file into the program.\n",
    "* Example: after `load_dotenv()`, you can use `os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")` to read your key.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. `os` (built-in Python library)\n",
    "\n",
    "* Lets Python access environment variables, files, and system functions.\n",
    "* Here, it‚Äôs used to fetch the token securely:\n",
    "\n",
    "  ```python\n",
    "  hf_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### What is an ‚ÄúEndpoint‚Äù in Hugging Face?\n",
    "\n",
    "An **endpoint** is like a **doorway (URL)** to a model hosted on Hugging Face.\n",
    "\n",
    "* A model repo (e.g., `Qwen/Qwen3-Coder-30B-A3B-Instruct`) is like a **book on a shelf**.\n",
    "* An inference **endpoint** is like the **librarian‚Äôs desk** ‚Äî you ask a question there, and it fetches the answer from the book.\n",
    "\n",
    "üëâ In short:\n",
    "\n",
    "* **Model** = the brain.\n",
    "* **Endpoint** = the online API that lets you talk to that brain.\n",
    "* **HuggingFaceEndpoint (LangChain)** = the Python connector that makes this communication easy.\n",
    "\n",
    "---\n",
    "\n",
    "### Why `ChatHuggingFace(llm=llm)`?\n",
    "\n",
    "* `HuggingFaceEndpoint` alone gives you raw completions from the model.\n",
    "* `ChatHuggingFace` wraps it into a **chat-style model**.\n",
    "* This gives you a simple, consistent API (`model.invoke(\"your question\")`) just like ChatGPT or Gemini.\n",
    "\n",
    "üëâ Analogy:\n",
    "\n",
    "* `HuggingFaceEndpoint` = direct phone line to the model (works, but clunky).\n",
    "* `ChatHuggingFace` = WhatsApp chat interface on top of that phone line (easy and user-friendly).\n",
    "\n",
    "---\n",
    "\n",
    "### Full Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "976b541c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Machine Learning in Simple Words:**\n",
      "\n",
      "Machine Learning is like teaching a computer to learn from examples, just like how you learn to recognize cats by seeing many pictures of cats.\n",
      "\n",
      "## How it works:\n",
      "- **You show the computer lots of examples** (like thousands of cat photos)\n",
      "- **The computer finds patterns** in those examples\n",
      "- **It learns to make predictions** or decisions on new, unseen examples\n",
      "\n",
      "## Real-life examples:\n",
      "- **Email spam detection** - learns to spot spam emails\n",
      "- **Netflix recommendations** - suggests movies you might like\n",
      "- **Google Maps** - predicts traffic and best routes\n",
      "- **Speech recognition** - understands what you're saying\n",
      "\n",
      "## The key idea:\n",
      "Instead of programming every single rule, you give the computer lots of examples and let it figure out the patterns on its own. It gets better over time as it sees more examples.\n",
      "\n",
      "Think of it as teaching by example rather than giving strict instructions!\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Get the Hugging Face token\n",
    "hf_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "# Choose the model (can be replaced with any other Hugging Face model that supports inference endpoints)\n",
    "repo_id = \"Qwen/Qwen3-Coder-30B-A3B-Instruct\"\n",
    "\n",
    "# Connect to the Hugging Face model endpoint\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    huggingfacehub_api_token=hf_token\n",
    ")\n",
    "\n",
    "# Wrap it as a chat model\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "# Ask a question\n",
    "result = model.invoke(\"what is machine learning in easy words\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce4e00b",
   "metadata": {},
   "source": [
    "## üìò Documentation for Hugging Face Pipeline with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995d3f37",
   "metadata": {},
   "source": [
    "This guide shows how to use Hugging Face models **locally (or via pipelines)** inside LangChain.\n",
    "Unlike `HuggingFaceEndpoint`, this approach does **not** require an API key, since it uses **pipelines** from the Hugging Face `transformers` library under the hood.\n",
    "\n",
    "---\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "1. Install required libraries:\n",
    "\n",
    "   ```bash\n",
    "   pip install langchain-huggingface transformers accelerate\n",
    "   ```\n",
    "\n",
    "2. No Hugging Face API token is needed ‚Äî models will be automatically downloaded the first time you run the code.\n",
    "\n",
    "---\n",
    "\n",
    "### Libraries & Classes Explained\n",
    "\n",
    "```python\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "```\n",
    "\n",
    "#### 1. `HuggingFacePipeline`\n",
    "\n",
    "* This connects LangChain to **Hugging Face pipelines** (a high-level abstraction in the `transformers` library).\n",
    "* A **pipeline** is like a **ready-to-use shortcut** for running specific ML tasks, such as:\n",
    "\n",
    "  * `text-generation` (generate text)\n",
    "  * `translation` (translate text)\n",
    "  * `summarization` (make summaries)\n",
    "  * `text-classification` (classify sentiment, spam, etc.)\n",
    "\n",
    "üëâ Instead of writing long boilerplate code, you just specify:\n",
    "\n",
    "* `model_id` ‚Üí which model to load (from Hugging Face Hub).\n",
    "* `task` ‚Üí what task you want to do.\n",
    "* `pipeline_kwargs` ‚Üí optional settings like `temperature`, `max_new_tokens`.\n",
    "\n",
    "#### 2. `ChatHuggingFace`\n",
    "\n",
    "* Just like in the endpoint example, this wrapper makes the pipeline behave like a **chatbot**.\n",
    "* It standardizes the API, so you can do:\n",
    "\n",
    "  ```python\n",
    "  result = model.invoke(\"your question\")\n",
    "  print(result.content)\n",
    "  ```\n",
    "\n",
    "instead of directly managing the raw pipeline output.\n",
    "\n",
    "---\n",
    "\n",
    "### Code Explanation\n",
    "\n",
    "```python\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "\n",
    "# Load the model using Hugging Face pipeline\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id='TinyLlama/TinyLlama-1.1B-Chat-v1.0',  # The model you want to use\n",
    "    task='text-generation',                         # Type of task (here: text generation)\n",
    "    pipeline_kwargs=dict(\n",
    "        temperature=0.5,    # Controls creativity (lower = more focused, higher = more random)\n",
    "        max_new_tokens=100  # Maximum number of tokens the model can generate\n",
    "    )\n",
    ")\n",
    "\n",
    "# Wrap it into a chat-friendly model\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "# Ask the model a question\n",
    "result = model.invoke(\"What is the capital of India\")\n",
    "\n",
    "# Print the result\n",
    "print(result.content)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ñ∂How it Works\n",
    "\n",
    "1. **Model Loading**\n",
    "\n",
    "   * When you run it the first time, Hugging Face downloads `TinyLlama/TinyLlama-1.1B-Chat-v1.0` automatically.\n",
    "   * After that, it‚Äôs cached locally for faster use.\n",
    "\n",
    "2. **Pipeline**\n",
    "\n",
    "   * `text-generation` pipeline is initialized.\n",
    "   * It knows how to take input text and generate continuations.\n",
    "\n",
    "3. **Chat Wrapper**\n",
    "\n",
    "   * `ChatHuggingFace` makes the interaction look like a simple chat interface.\n",
    "\n",
    "4. **Output**\n",
    "\n",
    "   * You get a human-readable string inside `result.content`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5ebf181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "What is the capital of India</s>\n",
      "<|assistant|>\n",
      "The capital of India is New Delhi.\n",
      "\n",
      "Source: https://www.india.travel/\n",
      "\n",
      "The official website of the Ministry of External Affairs of India also mentions the capital as New Delhi: https://mea.gov.in/stories/india-new-delhi-celebrates-100th-birthday-of-famous-writer-raaj-abhishek-bhattacharya\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "\n",
    "# Load the model using Hugging Face pipeline\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id='TinyLlama/TinyLlama-1.1B-Chat-v1.0',  # The model you want to use\n",
    "    task='text-generation',                         # Type of task (here: text generation)\n",
    "    pipeline_kwargs=dict(\n",
    "        temperature=0.5,    # Controls creativity (lower = more focused, higher = more random)\n",
    "        max_new_tokens=100  # Maximum number of tokens the model can generate\n",
    "    )\n",
    ")\n",
    "\n",
    "# Wrap it into a chat-friendly model\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "# Ask the model a question\n",
    "result = model.invoke(\"What is the capital of India\")\n",
    "\n",
    "# Print the result\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185f3110",
   "metadata": {},
   "source": [
    "## Does `HuggingFacePipeline` mean it‚Äôs only for **local** models?\n",
    "\n",
    "Yes ‚Äî **mostly correct**.\n",
    "\n",
    "* `HuggingFacePipeline` in **LangChain** is designed to wrap **Hugging Face‚Äôs `transformers` pipelines**, which are **local utilities**.\n",
    "* That means:\n",
    "\n",
    "  * The model is **downloaded from Hugging Face Hub** the first time.\n",
    "  * After that, it runs entirely on **your machine (CPU/GPU)**.\n",
    "  * No API call is made to Hugging Face Inference Endpoints.\n",
    "\n",
    "So if you‚Äôre using `HuggingFacePipeline`, you don‚Äôt need an API token.\n",
    "\n",
    "---\n",
    "\n",
    "## How is it different from `HuggingFaceEndpoint`?\n",
    "\n",
    "* **`HuggingFaceEndpoint`** ‚Üí connects to Hugging Face **Inference API (cloud endpoint)**.\n",
    "\n",
    "  * Requires API key (`HUGGINGFACEHUB_API_TOKEN`).\n",
    "  * Model runs on Hugging Face servers, not on your machine.\n",
    "\n",
    "* **`HuggingFacePipeline`** ‚Üí runs models **locally using `transformers` pipelines**.\n",
    "\n",
    "  * No API key needed.\n",
    "  * Requires your machine to have enough resources (RAM/VRAM).\n",
    "\n",
    "---\n",
    "\n",
    "## Analogy\n",
    "\n",
    "* **`HuggingFaceEndpoint`** = Ordering food from **Swiggy/Zomato** üçï (server prepares and sends to you).\n",
    "* **`HuggingFacePipeline`** = Cooking the food in your own kitchen üë®‚Äçüç≥ (runs on your machine).\n",
    "\n",
    "Both give you food, but the way you get it differs.\n",
    "\n",
    "---\n",
    "\n",
    "## Trade-offs\n",
    "\n",
    "* Use **`HuggingFacePipeline` (local)** if:\n",
    "\n",
    "  * You want **free usage**.\n",
    "  * You‚Äôre okay with downloading large models.\n",
    "  * Your machine has enough resources.\n",
    "\n",
    "* Use **`HuggingFaceEndpoint` (cloud)** if:\n",
    "\n",
    "  * You don‚Äôt have a powerful machine.\n",
    "  * You want to quickly try large models hosted on Hugging Face.\n",
    "  * You don‚Äôt mind using your API credits.\n",
    "\n",
    "---\n",
    "\n",
    "So yes ‚Äî `HuggingFacePipeline` is **meant for running Hugging Face models locally**, while `HuggingFaceEndpoint` is for **cloud execution**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326fc180",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
